{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f71847-f3b0-4883-a3d1-536ec37dc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 3 Fundamentals Of AI Shubh Sudan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d563dd74-dd2f-45cc-812f-bbb3765f2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import os # Added for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cc1a7c-725c-45de-93d9-bbd615a2b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from reviews_polarity_train.csv and reviews_polarity_test.csv...\n",
      "Data loaded successfully.\n",
      "Loading data from newsgroup_train.csv and newsgroup_test.csv...\n",
      "Data loaded successfully.\n",
      "1.A.Running Experiment: Movie Review Dataset (Q4 - No Smoothing)\n",
      "Now we build vocabulary and counting words...\n",
      "Vocabulary size = 2376\n",
      "Calculating Likelihood (Q4 - No Smoothing)\n",
      "Training done - now time to test\n",
      "\n",
      "Predicting on movie test set...\n",
      "\n",
      "--- Movie Review Results (Q4) ---\n",
      "Macro-Average Accuracy:  0.7700\n",
      "Macro-Average Precision: 0.7703\n",
      "Macro-Average Recall:    0.7700\n",
      "\n",
      "Confusion Matrix (Actual vs. Predicted):\n",
      "                     neg         pos\n",
      "------------------------------------\n",
      "         neg         118          32\n",
      "         pos          37         113\n",
      "1.B.Running Experiment: 20 Newsgroups Dataset (Q4 - No Smoothing)\n",
      "Now we build vocabulary and counting words...\n",
      "Vocabulary size = 26479\n",
      "Calculating Likelihood (Q4 - No Smoothing)\n",
      "Training done - now time to test\n",
      "\n",
      "Predicting on newsgroup test set...\n",
      "\n",
      "--- 20 Newsgroups Results (Q4) ---\n",
      "Macro-Average Accuracy:  0.6713\n",
      "Macro-Average Precision: 0.6807\n",
      "Macro-Average Recall:    0.6380\n",
      "\n",
      "Confusion Matrix (Actual vs. Predicted):\n",
      "                 atheism   christian        misc\n",
      "------------------------------------------------\n",
      "     atheism          88          39          15\n",
      "   christian          16         152          10\n",
      "        misc          14          47          48\n",
      "2.A.Running Experiment: Movie Review Dataset (Q7 - Laplace Smoothing)\n",
      "Now we build vocabulary and counting words...\n",
      "Vocabulary size = 2376\n",
      "Calculating Likelihood (Q7 - Laplace Smoothing, alpha=1)\n",
      "Training done - now time to test\n",
      "\n",
      "Predicting on movie test set (smoothed)...\n",
      "\n",
      "--- Movie Review Results (Q7) ---\n",
      "Macro-Average Accuracy:  0.8133\n",
      "Macro-Average Precision: 0.8136\n",
      "Macro-Average Recall:    0.8133\n",
      "\n",
      "Confusion Matrix (Actual vs. Predicted):\n",
      "                     neg         pos\n",
      "------------------------------------\n",
      "         neg         120          30\n",
      "         pos          26         124\n",
      "2.B.Running Experiment: 20 Newsgroups Dataset (Q7 - Laplace Smoothing)\n",
      "Now we build vocabulary and counting words...\n",
      "Vocabulary size = 26479\n",
      "Calculating Likelihood (Q7 - Laplace Smoothing, alpha=1)\n",
      "Training done - now time to test\n",
      "\n",
      "Predicting on newsgroup test set (smoothed)...\n",
      "\n",
      "--- 20 Newsgroups Results (Q7) ---\n",
      "Macro-Average Accuracy:  0.6457\n",
      "Macro-Average Precision: 0.7255\n",
      "Macro-Average Recall:    0.5875\n",
      "\n",
      "Confusion Matrix (Actual vs. Predicted):\n",
      "                 atheism   christian        misc\n",
      "------------------------------------------------\n",
      "     atheism          80          60           2\n",
      "   christian           3         171           4\n",
      "        misc          19          64          26\n",
      "All experiments for Q4 and Q7 are complete.\n"
     ]
    }
   ],
   "source": [
    "MOVIE_TRAIN_PATH = 'reviews_polarity_train.csv'\n",
    "MOVIE_TEST_PATH = 'reviews_polarity_test.csv'\n",
    "NEWS_TRAIN_PATH = 'newsgroup_train.csv'\n",
    "NEWS_TEST_PATH = 'newsgroup_test.csv'\n",
    "\n",
    "\n",
    "# --- Download NLTK resources one time ---\n",
    "for resource in ['punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}')\n",
    "    except LookupError:\n",
    "        print(f\"Downloading '{resource}' resource...\")\n",
    "        nltk.download(resource)\n",
    "\n",
    "\n",
    "\n",
    "# CLASS 1: NaiveBayes (for Q4 - NO Smoothing)\n",
    "\n",
    "\n",
    "#lets make a class naive bayes - which consists of different methods that would be helpful for us.\n",
    "#We will include the pre-processing of text within this class itself - to iterate the process whenever required..\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.log_priors = {}\n",
    "        self.log_likelihoods = {}\n",
    "        #Here we define a set as it stores unique entries and does not allow repetition of elements\n",
    "        self.vocabulary = set()\n",
    "        self.classes = set()\n",
    "        self.vocab_size = 0 #Size of vocab from dataset - initialized at 0\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        #Here we clean the textual data\n",
    "        \n",
    "        # --- FIX: Handle non-string (float/NaN) data ---\n",
    "        if not isinstance(text, str):\n",
    "            return [] # Return an empty list of tokens\n",
    "            \n",
    "        #Lowercasing the text\n",
    "        text = text.lower()\n",
    "        #Tokenizing - splitting sentences into words\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        #removing special characters,punctuations\n",
    "        cleaned_tokens = [word for word in tokens if word.isalpha()]\n",
    "        return cleaned_tokens\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        #Naive bayes trained using training data.\n",
    "        \n",
    "        # --- NOTE: This 'fit' method is for Q4 (no smoothing) ---\n",
    "        \n",
    "        num_docs = len(X_train)\n",
    "        self.classes = set(y_train)\n",
    "\n",
    "        #Here we calculate the parameters required in a naive bayes formula\n",
    "        #eg: prior prob, likelihood probability (Causation Approach)\n",
    "        for i in self.classes:\n",
    "            num_docs_in_class = sum(1 for label in y_train if label == i)\n",
    "            self.log_priors[i] = np.log2(num_docs_in_class / num_docs)\n",
    "\n",
    "            \n",
    "        #Now we build vocabulary\n",
    "        print(\"Now we build vocabulary and counting words...\")\n",
    "\n",
    "        word_counts_per_class = defaultdict(lambda: defaultdict(int))\n",
    "        total_words_per_class = defaultdict(int)\n",
    "\n",
    "        # This is where we loop through every single document to learn\n",
    "        for text, label in zip(X_train, y_train):\n",
    "            tokens = self.preprocess(text)\n",
    "\n",
    "            for word in tokens:\n",
    "                self.vocabulary.add(word)\n",
    "                #increment count(w,i)\n",
    "                word_counts_per_class[label][word] += 1\n",
    "                #increment total word count for the class\n",
    "                total_words_per_class[label] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size = {self.vocab_size}\")\n",
    "\n",
    "        print(\"Calculating Likelihood (Q4 - No Smoothing)\")\n",
    "        for i in self.classes:\n",
    "            self.log_likelihoods[i] = {}\n",
    "            total_word_count_class = total_words_per_class[i]\n",
    "\n",
    "            if total_word_count_class == 0:\n",
    "                print(f\"Warning: Class '{i}' has no words. Likelihoods will be -inf.\")\n",
    "                denominator = 1\n",
    "            else:\n",
    "                denominator = total_word_count_class\n",
    "\n",
    "            for word in self.vocabulary:\n",
    "                count_w_c = word_counts_per_class[i].get(word, 0)\n",
    "                numerator = count_w_c\n",
    "\n",
    "                # --- This is CRITICAL for no-smoothing ---\n",
    "                # We must handle log(0) which will crash the code.\n",
    "                if numerator == 0:\n",
    "                    self.log_likelihoods[i][word] = np.log2(1e-9) # Use a tiny log-prob\n",
    "                else:\n",
    "                    #calculating and storing likelihood\n",
    "                    self.log_likelihoods[i][word] = np.log2(numerator / denominator)\n",
    "\n",
    "        print(\"Training done - now time to test\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        #Predicts the class labels for a list of test documents.\n",
    "        #X_test: A list of raw text documents.\n",
    "        \n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            tokens = self.preprocess(text)\n",
    "            scores = {}\n",
    "            for i in self.classes:\n",
    "                if i not in self.log_priors:\n",
    "                    log_prob = np.log2(1e-9)\n",
    "                else:\n",
    "                    log_prob = self.log_priors[i]\n",
    "\n",
    "                for word in tokens:\n",
    "                    if word in self.vocabulary:\n",
    "                        if word in self.log_likelihoods[i]:\n",
    "                            log_prob += self.log_likelihoods[i][word]\n",
    "                \n",
    "                scores[i] = log_prob\n",
    "            \n",
    "            predicted_class = max(scores, key=scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# CLASS 2: NaiveBayesSmoothed (for Q7 - Laplace Smoothing)\n",
    "\n",
    "\n",
    "# We'll make a new class for this to keep things clean.\n",
    "# It's mostly a copy of the first class, but with a modified 'fit' method.\n",
    "class NaiveBayesSmoothed:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.log_priors = {}\n",
    "        self.log_likelihoods = {}\n",
    "        #Here we define a set as it stores unique entries and does not allow repetition of elements\n",
    "        self.vocabulary = set()\n",
    "        self.classes = set()\n",
    "        self.vocab_size = 0 #Size of vocab from dataset - initialized at 0\n",
    "        self.alpha = 1 # This is the '1' in (count + 1) for Laplace\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        #Here we clean the textual data\n",
    "        if not isinstance(text, str):\n",
    "            return [] # Return an empty list of tokens\n",
    "            \n",
    "        #Lowercasing the text\n",
    "        text = text.lower()\n",
    "        #Tokenizing - splitting sentences into words\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        #removing special characters,punctuations\n",
    "        cleaned_tokens = [word for word in tokens if word.isalpha()]\n",
    "        return cleaned_tokens\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        #Naive bayes trained using training data.\n",
    "        \n",
    "        # --- NOTE: This 'fit' method is for Q7 (WITH smoothing) ---\n",
    "        \n",
    "        num_docs = len(X_train)\n",
    "        self.classes = set(y_train)\n",
    "\n",
    "        #Here we calculate the parameters required in a naive bayes formula\n",
    "        #eg: prior prob, likelihood probability (Causation Approach)\n",
    "        for i in self.classes:\n",
    "            num_docs_in_class = sum(1 for label in y_train if label == i)\n",
    "            self.log_priors[i] = np.log2(num_docs_in_class / num_docs)\n",
    "\n",
    "            \n",
    "        #Now we build vocabulary\n",
    "        print(\"Now we build vocabulary and counting words...\")\n",
    "        word_counts_per_class = defaultdict(lambda: defaultdict(int))\n",
    "        total_words_per_class = defaultdict(int)\n",
    "\n",
    "        for text, label in zip(X_train, y_train):\n",
    "            tokens = self.preprocess(text)\n",
    "            for word in tokens:\n",
    "                self.vocabulary.add(word)\n",
    "                word_counts_per_class[label][word] += 1\n",
    "                total_words_per_class[label] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size = {self.vocab_size}\")\n",
    "\n",
    "        print(f\"Calculating Likelihood (Q7 - Laplace Smoothing, alpha={self.alpha})\")\n",
    "        for i in self.classes:\n",
    "            self.log_likelihoods[i] = {}\n",
    "            total_word_count_class = total_words_per_class[i]\n",
    "\n",
    "            # --- Q7 CHANGE 1: Denominator from Eq. 3.1 ---\n",
    "            # (Sum of word counts in class) + (alpha * |V|)\n",
    "            denominator = total_word_count_class + (self.alpha * self.vocab_size)\n",
    "\n",
    "            for word in self.vocabulary:\n",
    "                count_w_c = word_counts_per_class[i].get(word, 0)\n",
    "                \n",
    "                # --- Q7 CHANGE 2: Numerator from Eq. 3.1 ---\n",
    "                # (count(w, c)) + alpha\n",
    "                numerator = count_w_c + self.alpha\n",
    "\n",
    "                # Now we don't need to check for log(0) because\n",
    "                # the numerator can never be zero!\n",
    "                self.log_likelihoods[i][word] = np.log2(numerator / denominator)\n",
    "\n",
    "        print(\"Training done - now time to test\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # This method is identical to the one in the first class.\n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            tokens = self.preprocess(text)\n",
    "            scores = {}\n",
    "            for i in self.classes:\n",
    "                if i not in self.log_priors:\n",
    "                    log_prob = np.log2(1e-9)\n",
    "                else:\n",
    "                    log_prob = self.log_priors[i]\n",
    "                \n",
    "                for word in tokens:\n",
    "                    if word in self.vocabulary:\n",
    "                        # --- This is the one tiny change ---\n",
    "                        # What if a word is in our vocab, but *never*\n",
    "                        # appeared in class 'i'? We need to give it\n",
    "                        # the smoothed probability.\n",
    "                        if word in self.log_likelihoods[i]:\n",
    "                            log_prob += self.log_likelihoods[i][word]\n",
    "                        else:\n",
    "                            # This should not happen if vocab is built correctly,\n",
    "                            # but as a safety, we can assign the \"unknown word\" prob\n",
    "                            # which is (0 + alpha) / (Nc + alpha*|V|)\n",
    "                            total_word_count_class = total_words_per_class[i]\n",
    "                            denominator = total_word_count_class + (self.alpha * self.vocab_size)\n",
    "                            log_prob += np.log2(self.alpha / denominator)\n",
    "                \n",
    "                scores[i] = log_prob\n",
    "            \n",
    "            predicted_class = max(scores, key=scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# HELPER FUNCTIONS TO RUN EXPERIMENTS\n",
    "\n",
    "def load_data(train_csv_path, test_csv_path):\n",
    "    \"\"\"\n",
    "    A simple function to load our data from the CSV files.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {train_csv_path} and {test_csv_path}...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_csv_path)\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        train_df['Text'] = train_df['Text'].fillna('')\n",
    "        test_df['Text'] = test_df['Text'].fillna('')\n",
    "\n",
    "        X_train = train_df['Text'].tolist()\n",
    "        y_train = train_df['Label'].tolist()\n",
    "        \n",
    "        X_test = test_df['Text'].tolist()\n",
    "        y_test = test_df['Label'].tolist()\n",
    "        \n",
    "        print(\"Data loaded successfully.\")\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please make sure your CSV files are named correctly and have 'Text' and 'Label' columns.\")\n",
    "        return [], [], [], []\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Calculates and prints all the metrics required for Q4.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = sorted(list(set(y_true + y_pred)))\n",
    "    \n",
    "    correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
    "    accuracy = correct / len(y_true)\n",
    "    \n",
    "    metrics = {}\n",
    "    for c in classes:\n",
    "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == c and yp == c)\n",
    "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt != c and yp == c)\n",
    "        fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == c and yp != c)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        metrics[c] = {'precision': precision, 'recall': recall}\n",
    "        \n",
    "    macro_precision = sum(m['precision'] for m in metrics.values()) / len(classes)\n",
    "    macro_recall = sum(m['recall'] for m in metrics.values()) / len(classes)\n",
    "    \n",
    "    cm = defaultdict(lambda: defaultdict(int))\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        cm[yt][yp] += 1\n",
    "\n",
    "    print(f\"Macro-Average Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Macro-Average Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro-Average Recall:    {macro_recall:.4f}\")\n",
    "    print(\"\\nConfusion Matrix (Actual vs. Predicted):\")\n",
    "    \n",
    "    header = \" \" * 12\n",
    "    for c in classes:\n",
    "        header += f\"{c:>12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for actual_c in classes:\n",
    "        row = f\"{actual_c:>12}\"\n",
    "        for predicted_c in classes:\n",
    "            count = cm[actual_c].get(predicted_c, 0)\n",
    "            row += f\"{count:>12}\"\n",
    "        print(row)\n",
    "\n",
    "\n",
    "# --- Load Movie Data ---\n",
    "X_movie_train, y_movie_train, X_movie_test, y_movie_test = load_data(MOVIE_TRAIN_PATH, MOVIE_TEST_PATH)\n",
    "\n",
    "# --- Load News Data ---\n",
    "X_news_train, y_news_train, X_news_test, y_news_test = load_data(NEWS_TRAIN_PATH, NEWS_TEST_PATH)\n",
    "\n",
    "\n",
    "# === 1. Movie Review Dataset (Q4 - NO Smoothing) ===\n",
    "\n",
    "print(\"1.A.Running Experiment: Movie Review Dataset (Q4 - No Smoothing)\")\n",
    "\n",
    "if X_movie_train:\n",
    "    nb_movie = NaiveBayes()\n",
    "    nb_movie.fit(X_movie_train, y_movie_train)\n",
    "    print(\"\\nPredicting on movie test set...\")\n",
    "    y_movie_pred = nb_movie.predict(X_movie_test)\n",
    "    print(\"\\n--- Movie Review Results (Q4) ---\")\n",
    "    calculate_metrics(y_movie_test, y_movie_pred, nb_movie.classes)\n",
    "\n",
    "\n",
    "# === 2. 20 Newsgroups Dataset (Q4 - NO Smoothing) ===\n",
    "\n",
    "print(\"1.B.Running Experiment: 20 Newsgroups Dataset (Q4 - No Smoothing)\")\n",
    "\n",
    "if X_news_train:\n",
    "    nb_news = NaiveBayes()\n",
    "    nb_news.fit(X_news_train, y_news_train)\n",
    "    print(\"\\nPredicting on newsgroup test set...\")\n",
    "    y_news_pred = nb_news.predict(X_news_test)\n",
    "    print(\"\\n--- 20 Newsgroups Results (Q4) ---\")\n",
    "    calculate_metrics(y_news_test, y_news_pred, nb_news.classes)\n",
    "\n",
    "\n",
    "# === 3. Movie Review Dataset (Q7 - WITH Smoothing) ===\n",
    "\n",
    "print(\"2.A.Running Experiment: Movie Review Dataset (Q7 - Laplace Smoothing)\")\n",
    "\n",
    "if X_movie_train:\n",
    "    nb_movie_smooth = NaiveBayesSmoothed()\n",
    "    nb_movie_smooth.fit(X_movie_train, y_movie_train)\n",
    "    print(\"\\nPredicting on movie test set (smoothed)...\")\n",
    "    y_movie_pred_smooth = nb_movie_smooth.predict(X_movie_test)\n",
    "    print(\"\\n--- Movie Review Results (Q7) ---\")\n",
    "    calculate_metrics(y_movie_test, y_movie_pred_smooth, nb_movie_smooth.classes)\n",
    "\n",
    "\n",
    "# === 4. 20 Newsgroups Dataset (Q7 - WITH Smoothing) ===\n",
    "\n",
    "print(\"2.B.Running Experiment: 20 Newsgroups Dataset (Q7 - Laplace Smoothing)\")\n",
    "\n",
    "if X_news_train:\n",
    "    nb_news_smooth = NaiveBayesSmoothed()\n",
    "    nb_news_smooth.fit(X_news_train, y_news_train)\n",
    "    print(\"\\nPredicting on newsgroup test set (smoothed)...\")\n",
    "    y_news_pred_smooth = nb_news_smooth.predict(X_news_test)\n",
    "    print(\"\\n--- 20 Newsgroups Results (Q7) ---\")\n",
    "    calculate_metrics(y_news_test, y_news_pred_smooth, nb_news_smooth.classes)\n",
    "\n",
    "\n",
    "\n",
    "print(\"All experiments for Q4 and Q7 are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c578c-915c-464c-97a9-677c4393063a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
